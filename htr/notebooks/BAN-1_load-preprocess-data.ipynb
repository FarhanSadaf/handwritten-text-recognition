{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\ML\\Projects\\htr\\htr-github\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Location of 'project' directory.\n",
    "'''\n",
    "import os\n",
    "\n",
    "os.chdir('..')\n",
    "os.chdir('..')\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "\n",
    "# BASE_DIR is working directory for this notebook\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import h5py\n",
    "from htr.utils import split_data\n",
    "from htr.preprocessing import preprocess\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data (BanglaWriting dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANGLAWRITING_DIR = os.path.join(BASE_DIR, 'raw', 'BanglaWriting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_img_valid(img):\n",
    "    try:\n",
    "        if img.shape[0] > 0 and img.shape[1] > 0:\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af556ec83ee84f54b431ecf4c4b76bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=520.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid word image (*) on file 126_17_0.json\n",
      "Invalid word image (*) on file 126_17_0.json\n",
      "Invalid word image (*) on file 126_17_0.json\n",
      "Invalid word image (*) on file 128_16_1.json\n",
      "Invalid word image (শ্রেষ্ঠ) on file 183_16_0.json\n",
      "Invalid word image (*) on file 231_14_1.json\n",
      "Invalid word image (*) on file 234_17_1.json\n",
      "Invalid word image (*) on file 234_17_1.json\n",
      "Invalid word image (*) on file 234_17_1.json\n",
      "Invalid word image (*) on file 234_17_1.json\n",
      "Invalid word image (ব) on file 256_14_1.json\n",
      "Invalid word image (*) on file 68_12_0.json\n",
      "Invalid word image (*) on file 80_14_0.json\n",
      "\n",
      "# of samples: 21221\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "\n",
    "for root, dirs, files in os.walk(BANGLAWRITING_DIR):\n",
    "    for file in tqdm(files):\n",
    "        if file.endswith('.jpg'):\n",
    "            img = cv2.imread(os.path.join(root, file), cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                print(f'Couldn\\'t load image {file}')\n",
    "                continue\n",
    "            filename = file.split('.')[0]\n",
    "            \n",
    "        elif file.endswith('.json') and filename == file.split('.')[0]:\n",
    "            with open(os.path.join(root, file), encoding='utf-8') as jf:\n",
    "                data = json.load(jf)\n",
    "                for shape in data['shapes']:\n",
    "                    (xmin, ymin), (xmax, ymax) = shape['points']\n",
    "                    sample = {}\n",
    "                    sample['img'] = img[int(ymin):int(ymax), int(xmin):int(xmax)]\n",
    "                    sample['gt_text'] = shape['label']\n",
    "                    \n",
    "                    if not is_img_valid(sample['img']):\n",
    "                        print(f\"Invalid word image ({sample['gt_text']}) on file {file}\")\n",
    "                        continue\n",
    "\n",
    "                    samples.append(sample)\n",
    "                    \n",
    "print('# of samples:', len(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 20373\n",
      "Validation images: 424\n",
      "Test images: 424\n"
     ]
    }
   ],
   "source": [
    "dataset = split_data(samples, val_split_size=0.02, test_split_size=0.02)\n",
    "\n",
    "print('Train images:', len(dataset['train']))\n",
    "print('Validation images:', len(dataset['val']))\n",
    "print('Test images:', len(dataset['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess samples and save in a HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dataset_path = os.path.join(BASE_DIR, 'htr', 'data', 'BanglaWriting.hdf5')\n",
    "input_size = (1024, 128, 1)\n",
    "max_text_len = 128\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*HDF5 file structure*\n",
    "\n",
    "    BanglaWriting.hdf5\n",
    "       │    \n",
    "       ├───train\n",
    "       │   ├───imgs\n",
    "       │   └───gt_texts\n",
    "       ├───val\n",
    "       │   ├───imgs\n",
    "       │   └───gt_texts\n",
    "       ├───test\n",
    "           ├───imgs\n",
    "           └───gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(dataset_path, 'w') as hf:\n",
    "    for s in ['train', 'val', 'test']:\n",
    "        # Dummy imgs\n",
    "        hf.create_dataset(f'{s}/imgs',\n",
    "                          data=np.zeros(shape=(len(dataset[s]), input_size[0], input_size[1]), dtype=np.uint8),\n",
    "                          compression='gzip',\n",
    "                          compression_opts=9)\n",
    "        # Dummy ground truth texts\n",
    "        hf.create_dataset(f'{s}/gt_texts',\n",
    "                          data=[('c' * max_text_len).encode()] * len(dataset[s]), \n",
    "                          compression='gzip', \n",
    "                          compression_opts=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce4fc0c774b4f909084edb2909bd74f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bde395e537b445dad7f282a76475dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff699ac8c6f34457877cea68b037a9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for s in ['train', 'val', 'test']:\n",
    "    print(s)\n",
    "    for batch in tqdm(range(0, len(dataset[s]), batch_size)):\n",
    "        imgs = [preprocess(sample['img'], input_size) for sample in dataset[s][batch : batch + batch_size]]\n",
    "        gt_texts = [sample['gt_text'].encode() for sample in dataset[s][batch : batch + batch_size]]\n",
    "        \n",
    "        with h5py.File(dataset_path, 'a') as hf:\n",
    "            hf[f'{s}/imgs'][batch : batch + batch_size] = imgs\n",
    "            hf[f'{s}/gt_texts'][batch : batch + batch_size] = gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
